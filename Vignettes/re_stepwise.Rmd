---
title: "Report_Chapter8"
author: "Chiara Huwiler"
date: "2023-05-01"
output: html_document
---

## AGDS 8.5 Report Exercises


```{r}
# Set up
library(ggplot2)
library(tidyverse)
```

# Linear regression

```{r}
# Load data
df_regression <- readr::read_csv("./data/df_for_stepwise_regression.csv")
```

## Evaluation of all bivariate model

Task: An evaluation of all bivariate models (single predictor), implementing just steps 1-3 of the algorithm described in 8.2.3.1. This should be complemented by a visualisation and a brief discussion of the results. Steps in 8.2.3.1:

1.  Set the number of predictors to be considered to p = 1 The code repeats over each predictor in the dataset, so it considers only one predictor at a time.

```{r}
# 2. Fit all regression models with p predictors and compute their R^2

# Looping
# define a list of datasets or subsets of the same dataset
list_of_datasets <- list(df_regression)

# loop over the list of datasets
for (i in seq_along(list_of_datasets)) {
  
  # perform the regression analysis and save the results in a list
  storage_1 <- list()
  for (j in 1:length(names(list_of_datasets[[i]]))[-16]) {
    predictor <- names(list_of_datasets[[i]])[j]
    storage_1[[predictor]] <- lm(GPP_NT_VUT_REF ~ get(predictor), data = list_of_datasets[[i]])
  }
 # 3. Select the model with p predictors that achieves the highest R2^(best fitting model) and compute its AIC.
  
  # calculate the R2 values and save them in a vector
  r2values <- numeric(length(storage_1))
  for (k in 1:length(storage_1)) {
    r2values[k] <- summary(storage_1[[k]])$r.squared
  }
  
  # get the index of the best model based on R2 value
  max_index <- which.max(r2values)
  
  # fit the best model and calculate AIC
  best_predictor <- names(list_of_datasets[[i]])[max_index]
  best_model <- lm(GPP_NT_VUT_REF ~ get(best_predictor), data = list_of_datasets[[i]])
  AIC_value <- AIC(best_model)
  
  # plot the best model
  plot(best_model)
  
  # print the best predictor and its corresponding R-squared value and AIC
  cat("Best predictor candidate:", best_predictor, "\n")
  cat("Best R-squared value:", r2values[max_index], "\n")
  cat("AIC for best model:", AIC_value, "\n")
}

```

## Stepwise forward regression

Task: An implementation of stepwise forward regression, and a visualisation and discussion of its results.

```{r}
# Looping
# define a list of datasets
list_of_datasets <- list(df_regression)

# Loop over the list of datasets
for (i in seq_along(list_of_datasets)) {
  dataset <- list_of_datasets[[i]]
  
  # vector to store the selected variables
  all_vars_selected <- c()
  
  # Initialize AIC_current
  AIC_current <- Inf
  
  # Loop until the stopping criterion is met
  while (TRUE) {
    # Find the remaining predictors not yet included in the model
    remaining_predictors <- setdiff(names(dataset)[-16], all_vars_selected)
    
    # Stop the loop if no more predictors are available
    if (length(remaining_predictors) == 0) {
      break
    }
    
    # Initialize variables to track the best predictor and its AIC
    best_predictor <- NULL
    best_predictor_AIC <- Inf
    
    # Repeat over the remaining predictors
    for (pred_candidate in remaining_predictors) {
      # Specify the model formula with the current predictor candidate
      formula <- as.formula(paste("GPP_NT_VUT_REF ~", paste(c(all_vars_selected, pred_candidate), collapse = "+")))
      
      # Fit the model
      linmod_candidate <- lm(formula, data = dataset)
      
      # Calculate the AIC for the new model
      AIC_candidate <- AIC(linmod_candidate)
      
      # Store the AIC value if it is the lowest so far
      if (AIC_candidate < best_predictor_AIC) {
        best_predictor <- pred_candidate
        best_predictor_AIC <- AIC_candidate
      }
    }
    
    # Check if the improvement in AIC is below a threshold (here less than 2)
    if (best_predictor_AIC > AIC_current - 2) {
      break
    }
    
    # Add the best predictor to the selected variables
    all_vars_selected <- c(all_vars_selected, best_predictor)
    
    # Update the current model with the selected predictors
    formula <- as.formula(paste("GPP_NT_VUT_REF ~", paste(all_vars_selected, collapse = "+")))
    current_model <- lm(formula, data = dataset)
    
    # Update AIC_current for the current model
    AIC_current <- AIC(current_model)
  }
  
  # At this point, all_vars_selected will contain the selected predictors
  # current_model contain the final model
  
  # Print the selected predictors and their corresponding AIC value
  cat("Selected predictors:", paste(all_vars_selected, collapse = ", "), "\n")
  cat("AIC for final model:", AIC_current, "\n")
  
  # Plot the final model
  plot(current_model)
}

```

*Discussion of the results of the linear regression:*

The residuals vs. fitted model show if residuals have non-linear patterns. As it is not very linear here and seems to have some outliers, further investigation would need to be taken. All those outliners are also shown in the other three plots.

The normal Q-Q model shows points near (until around 2.8) the line, which speaks for a normal distribution. But the ends of the graph could speak for some extreme values. 

The scale location plot shows whether residuals are spread equally along the ranges of predictors. As the spread is more or less random, the variance seems to be equal.

The residual vs. leverage plot helps to identify points with a high influence on the model. When all points fall within the cook's distance, is there no influential observation, as it seems to be here.


