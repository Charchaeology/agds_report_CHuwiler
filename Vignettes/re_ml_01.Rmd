---
title: "Report_Chapter9"
author: "Chiara Huwiler"
output: 
  html_document:
    toc: true
---

## AGDS 9.4 Report Exercises

```{r}
# Set up
library(ggplot2)
library(tidyverse)
library(caret)
library(rsample)
library(recipes)
library(dplyr)
```

# Comparison of the linear regression and KNN models

## 1. Linear Regression Model and KNN

### Putting all together

```{r}
# Load Functions
source("./re_ml_01_functions.R")

# Load Data
daily_fluxes <- read_csv("./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv")

# Data cleaning
daily_fluxes |>
  ggplot(aes(x = GPP_NT_VUT_REF, y = ..count..)) + 
  geom_histogram()

# Data splitting
set.seed(1982)  
split <- rsample::initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)

# Model and pre-processing formulation, use all variables but LW_IN_F
pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = daily_fluxes_train |> drop_na()) |> 
  recipes::step_BoxCox(all_predictors()) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())


# Fit linear regression model
mod_lm <- function_lr()


# Fit KNN Modell
mod_knn <- function_knn()

```

```{r}
# Model evaluation
eval_model <- function_em

# linear regression model
eval_model(mod = mod_lm, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```

```{r}
# KNN
eval_model(mod = mod_knn, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```

## 2. Interpretation

*1. Why is the difference between the evaluation on the training and the test set larger for the KNN model than for the linear regression model?*

The linear regression model learns the overall relationship between the used variables and can make so decent predictions on new data. Whereas, the KNN model is not looking at the overall relationship, but at similar examples in the training data. This can lead to less accurate predictions on new data and here to a bigger difference between the evaluation on the training and the test set.

*2. Why is the does the evaluation on the test set indicate a better model performance of the KNN model than the linear regression model?*

This indicates that the KNN model is more accurate in predicting new data. For example because the KNN captures better complex patterns and non-linear relationships in the data, then the linear regression model. Furthermore it considers local patterns and can handle sparse data more effectively.

*3.* *How would you position the KNN and the linear regression model along the spectrum of the bias-variance trade-off?*

-   Linear Regression: have lower variance but can have higher bias if the relationship is not linear. They work well for linear patterns but can struggle with complex relationships.

-   KNN: have lower bias but higher variance. They can handle complex patterns but may overfit and be sensitive to specific training examples.

## 3. Visualization of temporal variations of observed and modeled GPP

```{r}

library(ggplot2)

# Filter out missing values
daily_fluxes <- daily_fluxes[complete.cases(daily_fluxes),]

# Add a column for the date
daily_fluxes$date <- as.Date(daily_fluxes$TIMESTAMP, origin = "1970-01-01")

# Create a dataframe with the observed and modeled GPP values
df <- data.frame(date = daily_fluxes$date,
                 observed = daily_fluxes$GPP_NT_VUT_REF,
                 modeled_lm = predict(mod_lm, newdata = daily_fluxes),
                 modeled_knn = predict(mod_knn, newdata = daily_fluxes))

# Melt the dataframe to long format
df <- reshape2::melt(df, id.vars = "date", 
                     variable.name = "model", 
                     value.name = "GPP")

# Plot the data
ggplot(df, aes(x = date, y = GPP, color = model)) +
  geom_line() +
  labs(x = "Date", y = "GPP", color = "Model") +
  theme_minimal()

```

# The role of K

## 1. Hypothesis

*Based on your understanding of KNN (and without running code), state a hypothesis for how the R\^2 and the MAE evaluated on the test and on the training set would change for k approaching 1 and for k approaching N (the number of observations in the data). Explain your hypothesis, referring to the bias-variance trade-off.*

Hypothesis: As k approaches 1, the model becomes more flexible, fits the training data closely, and may overfit. This leads to higher R\^2 and lower MAE on the training set. As k approaches N, the model becomes less flexible, averages predictions, and may oversimplify. This results in lower R\^2 and higher MAE on the training set.

## 2. Code

Task: Write code that splits the data into a training and a test set and repeats model fitting and evaluation for different values for k. Visualize results, showing model generalisability as a function of model complexity.

```{r}

knn_test1 <- function(k_values, daily_fluxes) {
  
  # Remove variables with zero variance
  selected_columns <- daily_fluxes %>%
    select(-ends_with("_SR"), -ends_with("_QC"), -G_F_MDS_QC, -RECO_SR, -RECO_SR_N)
  
  selected_columns <- selected_columns[complete.cases(selected_columns), ]
  
  # Initialize empty vectors to store results
  train_mae <- rep(NA, length(k_values))
  test_mae <- rep(NA, length(k_values))
  
  # Split the data into a training set and a test set
  set.seed(123)
  train_index <- createDataPartition(selected_columns$GPP_NT_VUT_REF, p = 0.8, list = FALSE)
  train <- selected_columns[train_index, ]
  test <- selected_columns[-train_index, ]
  
  # Convert train and test to data frames
  train <- as.data.frame(train)
  test <- as.data.frame(test)
  
  # Convert target variable to numeric
  train$GPP_NT_VUT_REF <- as.numeric(train$GPP_NT_VUT_REF)
  
  # Fit and evaluate the KNN model with the specified value of k
  for (i in seq_along(k_values)) {
    mod_knn <- tryCatch(
      train(
        GPP_NT_VUT_REF ~ .,
        data = train,
        method = "knn",
        trControl = trainControl(method = "none"),
        tuneGrid = data.frame(k = k_values[i]),
        preProcess = NULL
      ),
      error = function(e) NULL  # Return NULL if an error occurs
    )
    
    if (!is.null(mod_knn)) {
      train_pred <- predict(mod_knn, newdata = train)
      test_pred <- predict(mod_knn, newdata = test)
      train_mae[i] <- mean(abs(train_pred - train$GPP_NT_VUT_REF))
      test_mae[i] <- mean(abs(test_pred - test$GPP_NT_VUT_REF))
    }
  }
  
   # Find the index of the k value with the lowest test MAE
  best_index <- which.min(test_mae)
  
  # Plot the results
  plot <- ggplot() +
    geom_line(aes(x = k_values, y = train_mae), color = "blue", size = 1.5) +
    geom_line(aes(x = k_values, y = test_mae), color = "red", size = 1.5) +
    scale_x_continuous(name = "k") +
    scale_y_continuous(name = "MAE") +
    ggtitle("Model generalizability as a function of model complexity") +
    theme_classic()
  
  # Return the plot and test MAE
  return(list(plot = plot, test_mae = test_mae, optimal_k = k_values[best_index]))

}

k_values <- c(3, 5, 7)

result <- knn_test1(k_values, daily_fluxes)
print(result$plot)

```

Task: Write (some of your) code into a function that takes k as an input and and returns the MAE determined on the test set.

```{r}
# implemented in the code above
print(result$test_mae)
```

*Describe how a "region" of overfitting and underfitting can be determined in your visualisation*

Overfitting can be seen when the blue line (training MAE) descreases while the the red line (test MAE) increases or stays high. Whereas underfitting could be identified through a high training and test MAE and if there are similaritys across different values of k. Optimal complexity can be seen where the test MAE reaches its lowest value. The plot shows that this is a k value of 3 and as also can be seen below that the "optimal" k is 3. This k value tells where the model captures the underlying patterns in the data without over- or underfitting.

## 3. Optimal k?

Task: Is there an "optimal" k in terms of model generalisability? Edit your code to determine an optimal k.

The optimal k value is:

```{r}
# implemented in the code above
print(paste("Optimal k:", result$optimal_k))

```
